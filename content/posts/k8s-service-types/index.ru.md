---
title: "Типы сервисов Kubernetes в деталях: почему ClusterIP тоже можно балансировать нагрузку"
date: 2025-03-13T22:09:00+04:00
slug: "k8s-service-types-deep-dive"
draft: false
cover: "https://jiejue.obs.ap-southeast-1.myhuaweicloud.com/20250313221221836.webp"
tags:
  - "Kubernetes"
  - "сетевая инфраструктура"
  - "балансировка нагрузки"
  - "AKS"
---

В Kubernetes тип выбранной службы может быть важнее, чем вы думаете - он не только влияет на доступность приложений, но и напрямую связан с эффективностью архитектуры и стоимостью. Сегодня мы разберемся в часто недопонимаемом вопросе:** Почему балансировка нагрузки возможна и при использовании служб типа ClusterIP? **

<! --подробнее...

## Из запроса на архитектурную перестройку

Недавно, настраивая сетевую архитектуру кластера Azure AKS, я столкнулся с интересным вопросом. Изначально архитектура была такой:

```
App Gateway → Ingress → Internal Load Balancer (Service) → Pods
```.

а архитектор хотел изменить ее на:

```
App Gateway → Ingress → 直接到Pods (跳过LB)
```.

Это предложение поднимает основной, но критический вопрос:** Должен ли Ingress быть подключен к Pod через Service? **Если да, то можем ли мы использовать другой тип сервиса для оптимизации архитектуры?

## Типы сервисов в Kubernetes

В Kubernetes сервис - это уровень абстракции, который направляет трафик в поды. К распространенным типам сервисов относятся:

1. **ClusterIP** (по умолчанию): доступен только внутри кластера
2. **NodePort**: доступен через IP-адрес узла и указанный порт
3. **LoadBalancer**: создает внешний балансировщик нагрузки (например, Azure Load Balancer)
4. **ExternalName**: сопоставляет службу с внешним DNS-именем

Для вышеуказанных требований к изменению архитектуры мы выбрали тип ClusterIP вместо Internal Load Balancer. Это не только упрощает архитектуру, но и снижает стоимость. Но это приводит к более глубокому вопросу: **Как ClusterIP обеспечивает распределение трафика? **

## Механизмы балансировки нагрузки для ClusterIP

Многие ошибочно полагают, что ClusterIP не обладает возможностями балансировки нагрузки, поскольку не использует внешний балансировщик нагрузки. На самом деле ClusterIP обеспечивает балансировку нагрузки, но с помощью другого механизма.

На следующей диаграмме показано, как служба ClusterIP распределяет трафик:

```mermaid
flowchart TD
    subgraph "Kubernetes集群"
        subgraph "kube-proxy层"
            KP[kube-proxy组件] --> |3.维护IPVS/iptables规则| IPT[IPVS/iptables规则]
        end
        
        API[API Server] --> |1.监听Service/Pod变化| KP
        ETCD[etcd] <--> |存储集群状态| API
        
        CIP[Service ClusterIP\n10.96.0.10:80] --> |4.流量转发| IPT
        
        subgraph "后端Pod"
            IPT --> |5a. 转发请求| POD1[Pod 1<br>10.244.1.2:8080]
            IPT --> |5b. 转发请求| POD2[Pod 2<br>10.244.2.3:8080]
            IPT --> |5c. 转发请求| POD3[Pod 3<br>10.244.3.4:8080]
        end
    end
    
    ING[Ingress Controller] --> |2.请求服务| CIP
```.

### Как это работает в деталях

1. **Создание ресурса сервиса
   - Когда создается служба ClusterIP, Kubernetes назначает виртуальный IP.
   - Этот IP виден только в пределах кластера

2. **Роль kube-proxy**
   - На каждом узле запускается компонент kube-proxy
   - Прослушивать изменения в службах и конечных точках на сервере API
   - Обновление сетевых правил на узлах

3. **Механизм переадресации трафика** **Механизм переадресации трафика
   - **iptables mode** (по умолчанию): создание правил iptables для случайной пересылки трафика
   - **IPVS mode**: использование модуля Linux kernel IP Virtual Server для обеспечения большего количества алгоритмов балансировки нагрузки
   - **режим пользовательского пространства**: старый режим, низкая производительность, не часто используется

Представьте, что kube-proxy - это как сетевой "датчик", постоянно отслеживающий изменения сервисов и подсистем в кластере и динамически обновляющий правила переадресации.

## iptables vs IPVS: две реализации балансировки нагрузки

### режим iptables

В режиме iptables по умолчанию kube-proxy создает правила примерно следующим образом:

```bash
# 伪代码示例
-A KUBE-SERVICES -d 10.96.0.10/32 -p tcp -m tcp --dport 80 -j KUBE-SVC-XXX
-A KUBE-SVC-XXX -m statistic --mode random --probability 0.33333 -j KUBE-SEP-POD1
-A KUBE-SVC-XXX -m statistic --mode random --probability 0.50000 -j KUBE-SEP-POD2
-A KUBE-SVC-XXX -j KUBE-SEP-POD3
```.

Эти правила обеспечивают случайное распределение трафика с помощью вероятностного распределения.

### Режим IPVS

IPVS (IP Virtual Server) - это модуль в ядре Linux, используемый для реализации балансировки нагрузки на транспортном уровне. По сравнению с iptables:
- Использует хэш-таблицы в качестве базовой структуры данных, более эффективен.
- Поддерживает больше алгоритмов балансировки нагрузки:
  - rr: опрос (по умолчанию)
  - lc: наименее подключенный
  - dh: хэш назначения
  - sh: хэш источника
  - sed: минимальная ожидаемая задержка
  - nq: никогда не ставить в очередь

Чтобы включить режим IPVS, задайте его в конфигурации kube-proxy:

```yaml
mode: ipvs
```.

## Настройте ClusterIP для сродства сессий

Если вы хотите, чтобы запросы от одного и того же клиента всегда поступали к одному и тому же боду, вы можете настроить SessionAffinity:

```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: MyApp
  sessionAffinity: ClientIP  # 基于客户端IP的会话亲和性
  ports:
  - port: 80
    targetPort: 8080
```

Обратите внимание, что в настоящее время Kubernetes поддерживает только два значения sessionAffinity: `None` (по умолчанию) и `ClientIP`.

## ClusterIP vs LoadBalancer: как выбрать?

**Преимущество LoadBalancer**:
- Возможность прямого доступа из внешней сети.
- Может предлагать более продвинутые функции (завершение SSL, расширенный мониторинг и т. д.)

Преимущество **ClusterIP**:
- Позволяет избежать дополнительных расходов на балансировщик нагрузки облачного провайдера
- Сокращение количества уровней переходов по сетевому маршруту
- Подходит для внутренних служб

**Рекомендация по выбору**:
- Если сервису требуется прямой внешний доступ → LoadBalancer
- Если служба требует только внутреннего доступа или уже имеет Ingress в качестве точки входа → ClusterIP

## Практическое применение: оптимизация архитектуры Azure AKS

Для решения исходной задачи мы заменили внутренний балансировщик нагрузки на службу типа ClusterIP:

```yaml
apiVersion: v1
kind: Service
metadata:
  name: api-service
spec:
  type: ClusterIP  # 默认类型，不需要外部LB
  selector:
    app: api-app
  ports:
  - port: 80
    targetPort: 8080
```.

На эту службу ссылаются в конфигурации Ingress:

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: api-ingress
spec:
  rules:
  - http:
      paths:
      - path: /api
        pathType: Prefix
        backend:
          service:
            name: api-service
            port:
              number: 80
```

Таким образом, мы успешно достигли архитектурной простоты: `App Gateway → Ingress → ClusterIP Service → Pods`, сохранив при этом функциональность балансировки нагрузки.

## Резюме

Сетевой дизайн Kubernetes демонстрирует свою элегантность, предоставляя различные уровни сетевой абстракции в зависимости от требований. clusterIP - это не только простейший тип сервиса, но и мощный инструмент для балансировки нагрузки внутренних сервисов. Он распределяет трафик на уровне узла через kube-proxy, избавляя от необходимости полагаться на внешние балансировщики нагрузки.

Понимание этих базовых механизмов поможет нам принимать более разумные архитектурные решения, чтобы обеспечить стабильность системы и контролировать расходы на инфраструктуру.

**Есть ли возможности для оптимизации и в архитектуре вашего сервиса Kubernetes? Стоит задуматься о выборе типа сервиса. **

---

**Дополнительная информация**:
- [Официальная документация Kubernetes: Service](https://kubernetes.io/docs/concepts/services-networking/service/)
- [Глубокое погружение в схему kube-proxy](https://kubernetes.io/docs/concepts/services-networking/service/#proxy-mode-ipvs)
